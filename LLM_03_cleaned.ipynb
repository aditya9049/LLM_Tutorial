{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "40ef87fdc4654924a13a269afc05688d",
            "d8dcc63aaa1c4c51bec33ca75b60bdb1",
            "c6cbe02ad2ed4fc0814bef192961fee7",
            "5cb63eac349348c59682cc8ed9ba1808",
            "e638a980e16c489c9e51964b6a214194",
            "44e3cc70dd02483593e11e70e59612d9",
            "0f8cd02426a2476986d1c003aeb26da0",
            "7fa9bd8b42894356a5b312e31ca87f52",
            "9a014f1654ec4db4ad8dc89d05c0d34e",
            "3a1b9c1e1ac24698ad2df9d65d843632",
            "ef93c97850484a08b48cecb5c35be80b",
            "f143c7d037a4464fbe06047b9a4ac17d",
            "bdd39fd24d5b4237b4d7989e43ce048c",
            "a14de1dcd5e1452f9de89ba54bde95cc",
            "ac4a40f96c744871bf310e8952856300",
            "b9a6ba8484fc4289a36ca1f05b69d214",
            "b5e7213f4e1b415ea98875104783a885",
            "fc572aed461c499089492b08b8ddbf18",
            "e73e9c134bc34147ba27f69c0656d768",
            "0dd9c16728214242af14a0689188ce05",
            "1fa58754b67d4202867a5995a12a69bc",
            "6360dbb8d32d4c899f43deb577a2cb7e",
            "12fd64fdfe1b4945968eea9972b11d29",
            "8967e3afbc8a4b1a89c98e13b8f3d397",
            "fdab6477c0c94baf8e6eabac16b48f5d",
            "8622500adb744ef49ed9c900a3463ce1",
            "061c10d0b617456a9cb8de0c028246c1",
            "434c5318a0a14d28a070984333ac5aa2",
            "079e61c2db254a349471451bee9d869d",
            "28eb2ab3d2354d38922a07288509f2b3",
            "bc2de5f29ff24d948ed79943bebe0d99",
            "6482514abd9c4c9dafa77a215d22d7f4",
            "2c3fd353370140dcaedab64a12c1a6af",
            "3d79e1dbbd594e8abb62a8446e0b049a",
            "78763ab521124d42869fe058585227a6",
            "5819a262d3864b2cad17f73c11fac033",
            "9075abf514b74743bf069c5a9e92c8c8",
            "b460d9a6f27a4f708ddadeca1a28f695",
            "70219737ac204f92a3a1d23d7ecabd6b",
            "7bdca41256a241148c732540422cb063",
            "67157f5253034638b0c25a36a332eb3b",
            "afb687ab855e4e53bc9e430d762acfb2",
            "a8de1010b71044158c2ffe684635c768",
            "05347ba52d1146268d316956f7b64fcb",
            "56a0e82d88d54217afad3bd4984142c5",
            "7d5a125086154ae79540482c66fa2b3a",
            "0471bcbf5e28424e956f49d7ebfa4b7d",
            "79e96002215e46e0853c6ade1e8bd24e",
            "253b638bd5514f83b07f8b0c928ccb46",
            "c824f0b22f054a509ee19ab5252159ea",
            "d6f41a401ff1467ca9ac04bce4d1e895",
            "fe30536c06db4cf7838a6164ed16242c",
            "57152b78318843f2a9379be2b6b96297",
            "ed3782cb98e34c98920e882df884cf11",
            "87c36b0778b74a29aafb00826d2fa299",
            "0f1ef9bf774e4736aa5e5d516c12e39d",
            "e2ca3d6a424b4e06a9b71b6efb3a7428",
            "d8416a1bbcb844318da45595e72ac69f",
            "fc88e456693c4080b0428549ba516fa3",
            "d13f58f1264d431baad35ef19cad70a1",
            "fb483d9f84354e49a7f1b8558f1762b4",
            "296dd74cebd94835bd02669a3f719095",
            "5d5475e0d38c4aa588b824a2b1b2533d",
            "2fec80825a8b4dd69204e1f70be7b5a2",
            "401feee6741647db95ebb10d36a479c9",
            "34f34ad056f04f4284e80f61fa19adf5",
            "3ecc97ba5c4149889e06987bb12f96b5",
            "43a40d9a74c2402e8deda6bb48620e2a",
            "9810ace3ac6d4268b920aff8015f1535",
            "ccd4bb96e0ce4834aeb1ed0e44a5ecf9",
            "0fd521980ade4ab08d743090ae4d3485",
            "028338472d7545e686efe0d440970269",
            "f123b298965f4067ad452db848a7fc73",
            "b6ee4c21d9b248e1b18ac1d40c59458f",
            "b9e35538abed424193c25e918476101d",
            "ff0c968b0cca44cab7eacbd13dfc35d8",
            "f6b6296f181f49419f837a4c2a8f1a69",
            "81f369cca6924928b584cb5bfe8d3e92",
            "080c7541862742ad8be4e6e8e0762d6f",
            "1cd24a80305b452ba704794a0669f5e5",
            "d87b58a0f55f48568771fea6437e9c60",
            "7296b4b3fe6b47559842a0c6491e0a99",
            "5958a23559bd4978b98c3841686c36e6",
            "49a4b72970c94aea90bc303b8d0fd872",
            "b1842d24afad4e0abab50d95b50e94e4",
            "24fce1f0be214fbb9db56dc78fe07e27",
            "af22fa725ee647b28a88ca307aa83d02",
            "224282179c4243fb8d491b65d8b237d0",
            "d06040011ea04291812876edd2da99a8",
            "69829d758ef84f5fb10d132f9b04362d",
            "cd15a74b631341c0aea32ebed599ea48",
            "728324c51eb0451e8c498a47b293e797",
            "2ff178d5bedf4f9d92d8d05d6757f4dd",
            "ca95793e355a4b6b859e448880e0d108",
            "384dd7b2ae1145eb804600b6b63bd7f4",
            "ffb7f10acdcb4158beef59d765968422",
            "d180bfaa50574365b52f6ae0c33e1ae5",
            "b6ecfa9cc554467fb530cf47d2c5faf9",
            "72802ef6fa79488ab8a3ab6d51de31e9",
            "ec231e1fe03245b189ef78fe4918b89f",
            "a39f7c337b5741f4a3a16539f795b485",
            "3148109392ba4c16b7e0132c36be004c",
            "13e94662bc04448dbcc165dbef0a583c",
            "9d90e3ce73824f2e99575c1d2a7c41f4",
            "8cab4f11a9ce4650aa9505eb9f096d23",
            "d0f6a01616f347338e527bece0bbded8",
            "17533e49486941ca8f5cb4dbaca55f71",
            "012bb39b90ae45c8a0702b0a5e97a000",
            "bcdca061cec34001a73d31d400bec80b",
            "ad9f9006daef4a7684df143dd92d3c0f",
            "b607d3dc83354f968cfea0e3ee2bfa87",
            "a7119e4452dd4c3c8dccc25b0fdafab2",
            "f5820039165f4dd28a450398ec0ef015",
            "fd46eb83b1ab45bf915b262286c8006f",
            "a1ca5ba6ca54433da384c285d5908fcd",
            "6f283eb268f644cc902cb50c652621bf",
            "7a942634aae2488ebcd34580944f3ad9",
            "a2443598f09d4662aca7944ad4b8ee29",
            "17a4b609d1e04a66bca625102391a828",
            "1a706334224340d3a84b8eb721d39064",
            "7647f34457c14952a43de5e26924ea39",
            "8fff069404574ae6b18ecd5b9a75fc09",
            "1e2ea17c5e89402887d5ac63d9d709fe",
            "3157cb58587749f9b0cbd963fa0a1b48",
            "99c8cc405f5949cd8f1f1de25e1d89b4",
            "9aa20c82ff334c09a7f0f78d34c55f2c",
            "7ab0ea6d1f8d40ada5e8ab5bb588cc91",
            "c5ad0fabf2554f65a4d157bdb1ddefd8",
            "08cbed93ba7349c7b2ca4b38612909df",
            "7c3412057f0f44bdb0bbb4089852a6dd",
            "bbb92040d02e42f4976186ce69056f88",
            "fd84e1fe26774b3aa8388f5ed380421b"
          ]
        },
        "id": "EHl0zo_9TEXB",
        "outputId": "e0861118-f3b1-43ea-9d56-d2070121a4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40ef87fdc4654924a13a269afc05688d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f143c7d037a4464fbe06047b9a4ac17d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12fd64fdfe1b4945968eea9972b11d29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d79e1dbbd594e8abb62a8446e0b049a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56a0e82d88d54217afad3bd4984142c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f1ef9bf774e4736aa5e5d516c12e39d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ecc97ba5c4149889e06987bb12f96b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81f369cca6924928b584cb5bfe8d3e92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d06040011ea04291812876edd2da99a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec231e1fe03245b189ef78fe4918b89f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b607d3dc83354f968cfea0e3ee2bfa87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fff069404574ae6b18ecd5b9a75fc09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZTHrGbFWqw2",
        "outputId": "92faae4c-3334-4f45-db98-546cfe5a591f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my sincerest apologies for the unfortunate incident that occurred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsLUq74gYVu7",
        "outputId": "fcd9c448-3731-48b6-a231-17f62442d1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a single token from the probability distribution (sampling / decoding)**bold text**"
      ],
      "metadata": {
        "id": "0RJC5t_jZt0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# Get the output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ],
      "metadata": {
        "id": "bEAao7ZvXVCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YvQSLctTXVnk",
        "outputId": "b2aeb1ab-eef1-49d2-8f2b-35e218026744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AD1GtQEYOHC",
        "outputId": "e4418b64-5da2-4153-cde5-d94e3f39a74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoExw8vTYcTp",
        "outputId": "ae4f4834-3669-481c-f3c6-52992438b9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speeding up generation by caching keys and values"
      ],
      "metadata": {
        "id": "_YnNTkP0Z3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "KFjtHjdjZEqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfuXn2zJYfuG",
        "outputId": "e38a365d-2325-415b-cc48-87f4d9d61398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35 s \u00b1 184 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SV1w4oUZDV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-c9JvUZJY-zg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}